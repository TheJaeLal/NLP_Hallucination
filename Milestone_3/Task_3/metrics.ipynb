{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair the response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"McGill-NLP/FaithDial\")\n",
    "print(len(dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with GT Response in files that didn't have it.. \n",
    "input_file = \"T5_gen_WoW.txt\" # #input_['knowledge'],input_['prompt'],resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, 'r') as f:\n",
    "    input_data = f.read().split(\"\\n\")\n",
    "\n",
    "# pipe delimited knowledge, history and pred_response\n",
    "input_data = [x.split(\"|\") for x in input_data]\n",
    "\n",
    "for idx, test_sample in enumerate(dataset['test']):\n",
    "    knowledge, prompt, pred_resp = input_data[idx]\n",
    "    original_resp = test_sample['original_response']\n",
    "    gt_resp = test_sample['response']\n",
    "    input_data[idx].extend([original_resp, gt_resp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = [[cell if cell is not None else '' for cell in row] for row in input_data]\n",
    "data_dump = \"\\n\".join(['|'.join(res) for res in all_results])\n",
    "\n",
    "with open(\"T5_gen_WoW.txt\", 'w') as f:\n",
    "    f.write(data_dump)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [input_['knowledge'], input_['prompt'], resp,\n",
    "#                          test_sample['original_response'], test_sample['response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3539"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from bert_score import score as bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_rouge = load_metric(\"rouge\")\n",
    "metric_bleu = load_metric(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file = \"T5_gen_WoW.txt\"\n",
    "# input_file = \"T5_edit_FaithDial_khorr.txt\"\n",
    "input_file = \"T5_gen_FaithDial.txt\"\n",
    "with open(input_file, 'r') as f:\n",
    "    input_data = f.read().split(\"\\n\")\n",
    "\n",
    "# pipe delimited knowledge, history and pred_response\n",
    "input_data = [x.split(\"|\") for x in input_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['knowledge', 'prompt', 'model_response', 'original_response', 'gt_response']\n"
     ]
    }
   ],
   "source": [
    "print(input_data[0])\n",
    "# Drop the Header\n",
    "input_data = input_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3539/3539 [00:00<00:00, 6051.65it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "\n",
    "tok_predictions = []\n",
    "tok_references = []\n",
    "\n",
    "for fields in tqdm(input_data):\n",
    "    _, _, pred_resp, _, gt_resp = fields\n",
    "    tok_predictions.append(tokenizer.tokenize(pred_resp))\n",
    "    tok_references.append([tokenizer.tokenize(gt_resp)])\n",
    "    predictions.append(pred_resp)\n",
    "    references.append([gt_resp])\n",
    "\n",
    "    # print(len(knowledge), len(history), len(pred_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3539, 3539)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions), len(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm not sure, but they're currently in New York City, New York, Los Angeles, and Miami Beach.\", \"I'm not sure, but they are currently in New York City, East Hampton, and Miami Beach.\"]\n",
      "[[\"I don't know how good they are, but Dylan's Candy Bar has a chain of candy shops in various cities.\"], [\"I don't know, really, but they also are a supplier of candy.\"]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[:2])\n",
    "print(references[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bleu = metric_bleu.compute(predictions=tok_predictions, references=tok_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_rouge = metric_rouge.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "_, _, bert_score_f1 = bert_score(\n",
    "    predictions, \n",
    "    references, \n",
    "    lang=\"en\",\n",
    "    model_type=\"distilbert-base-uncased\",\n",
    "    return_hash=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.165\n",
      "ROUGE-1, ROUGE-2, ROUGE-L: 0.405, 0.190, 0.351\n",
      "BERTScore: 0.821\n"
     ]
    }
   ],
   "source": [
    "print(f\"BLEU: {res_bleu['bleu']:.3f}\")\n",
    "print(f\"ROUGE-1, ROUGE-2, ROUGE-L: {res_rouge['rouge1'].mid.fmeasure:.3f}, {res_rouge['rouge2'].mid.fmeasure:.3f}, {res_rouge['rougeL'].mid.fmeasure:.3f}\")\n",
    "print(f\"BERTScore: {bert_score_f1.mean().item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_results = []\n",
    "# for test_sample in tqdm(dataset['test'], total=len(dataset['test'])):\n",
    "#     input_ = {'knowledge': test_sample['knowledge'],\n",
    "#             'prompt': test_sample['history'][-1]}\n",
    "    \n",
    "#     resp = infer(model, max_input_length=MAX_INPUT_LENGTH, max_output_length=100, device=DEVICE, **input_)\n",
    "#     all_results.append([input_['knowledge'], input_['prompt'], resp,\n",
    "#                          test_sample['original_response'], test_sample['response']])\n",
    "\n",
    "# # clean_all-results\n",
    "# all_results = [[cell if cell is not None else '' for cell in row] for row in all_results]\n",
    "# data_dump = \"\\n\".join(['|'.join(res) for res in all_results])\n",
    "\n",
    "# with open(\"T5_edit_FaithDial_khorr.txt\", 'w') as f:\n",
    "#     f.write(data_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most dissimilarity??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_df = pd.read_csv(\"T5_edit_FaithDial_khorr.txt\", sep=\"|\")\n",
    "drg_df = pd.read_csv(\"T5_gen_FaithDial.txt\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       I'm not sure, but they're currently in New Yor...\n",
       "1       I'm not sure, but they are currently in New Yo...\n",
       "2          They have 7,000 candies from around the world.\n",
       "3       I'm not sure, but it's owned by Dylan Lauren, ...\n",
       "4       Ah, OK. Did you know ancient Greece advised at...\n",
       "                              ...                        \n",
       "3534    Yeah, and they play at the United Center, whic...\n",
       "3535    I'm a bot, so I can't sing. I know it can be d...\n",
       "3536    Sure, you can sing with or without accompanime...\n",
       "3537    That's nice. Do you have an excellent singing ...\n",
       "3538    I'm a bot, so I don't have favorites. I know t...\n",
       "Name: model_response, Length: 3539, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drg_df['model_response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       I don't have any preferences. I know that ther...\n",
       "1            I don't know, but they are a candy supplier.\n",
       "2       I don't know but it has 7000 candies from arou...\n",
       "3                   I can't say but Dylan Lauren owns it.\n",
       "4       I see, did you know that athletes in ancient G...\n",
       "                              ...                        \n",
       "3534    Yes, they played their home games at the Unite...\n",
       "3535    I'm a bot and can't sing. I do know that it ca...\n",
       "3536    Yeah, musical instruments can be accompanied b...\n",
       "3537    I see, did you know that excellence in singing...\n",
       "3538    I'm a bot, so I can't sing. What I know is tha...\n",
       "Name: model_response, Length: 3539, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fre_df['model_response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
